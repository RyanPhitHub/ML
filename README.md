# ML-Model
[Here's the form we used to gather data](https://docs.google.com/forms/d/e/1FAIpQLScsVpWJKa3I6Y5-KlMmNBtE_Hkj0mYuOlSISLbmcTFs8Ii6Fw/viewform?usp=sf_link)  

### Reflections:

Ryan:
We definitely do not have enough data in order to make a remotely accurate prediction model- as seen by the difference in loss between our validation and test data. Also it is obvious that human error could be involved, since maybe some of the features we chose actually have very weak correlation to people's compatibilities. Or, the test we use to determine what people's "true" compatibility is, is inaccurate. Also, since we chose how to weigh the points for each question for the "objective" portion of the google form, our point/scoring system way be weighted incorrectly.  

Nadim:
Of the data we do have, there are some parameters in it eg. Landslide votes for favourite food, grade, all of which effect our machine learning as it would be trained on only one set of data due to a lack of variety. In my opinion, I don't know how accurate the ML model would be since it is taking categorical data and turning it into numerical data, which the model would calculate in absurd ways we do not know of- especially with the very little entries of data. This could be a problem for the accuracy of the model since there's no level of grading for these outputs.  

In conclusion, our biggest error with this project was the lack of data, which we could not control. We also used very impartial data which, as mentioned in the previous paragraph, could lead to lack of variety in the ML model.
